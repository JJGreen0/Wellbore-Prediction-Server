{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2312671-afa9-40c9-b3d4-664ec9d40eab",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWLC_MUD_LOG_INTERPOLATED.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m target_to_feature \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGR\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mICGRC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWOBA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEXM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDXC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTQA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTHDIGITAL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROP\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROP\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDXC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEXM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROPA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTHDIGITAL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFRACTUREGRADNT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBDTI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKREV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBDDI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \n\u001b[0;32m     28\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1456\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\multiarray.py:1131\u001b[0m, in \u001b[0;36mputmask\u001b[1;34m(a, mask, values)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \n\u001b[0;32m   1172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics     import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('WLC_MUD_LOG_INTERPOLATED.csv')\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "target_to_feature = {\n",
    "    'GR': ['GR', 'ICGRC', 'TH', 'WOBA', 'NEU', 'DEXM', 'DXC', 'U', 'TQA', 'LTHDIGITAL', 'ROP'],\n",
    "    'ROP': ['ROP', 'DXC', 'DEXM', 'ROPA', 'U', 'LTHDIGITAL', 'FRACTUREGRADNT', 'BDTI', 'KREV', 'BDDI', 'Unnamed: 0'],\n",
    "    'TQA': ['LTHDIGITAL', 'WOBA', 'PWPA', 'KREV', 'BDTI', 'OVERBDNGRADNT', 'Unnamed: 0.1', 'DVER', 'BDDI', 'MTOA'],\n",
    "    'RPMT': [\"RPMS\", \"RPMT\", \"MFIA\", \"SPPA\", \"C1C2\", \"ACS\", \"ARM48P\", \"ARM16P\", \"AC\", \"RSHA\", \"HKLX\", \"RMED\"],\n",
    "    'BDTI': [\"BDTI\", \"KREV\", \"Unnamed: 0\", \"BDDI\", \"DVER\", \"OVERBDNGRADNT\", \"PWPA\", \"FRACTUREGRADNT\", \"PPE\", \"MTOA\", \"LTHDIGITAL\"],\n",
    "    'KREV': [\"KREV\", \"BDTI\", \"Unnamed: 0\", \"BDDI\", \"DVER\", \"OVERBDNGRADNT\", \"PWPA\", \"FRACTUREGRADNT\", \"PPE\", \"MTOA\", \"LTHDIGITAL\"],\n",
    "    'DXC': ['DEXM', 'ROPA', 'WOBA', 'ROP', 'U', 'LTHDIGITAL', 'ICGRC', 'BDTI', 'KREV', 'FRACTUREGRADNT', 'ETHA'],\n",
    "    'LTHDIGITAL': [\"LTHDIGITAL\", \"TVA\", \"FRACTUREGRADNT\", \"PPE\", \"BDDI\", \"Unnamed: 0\", \"Unnamed: 0.1\", \"DVER\", \"PWPA\", \"KREV\", \"BDTI\", \"OVERBDNGRADNT\"],\n",
    "    'PPE': [\"PPE\", \"DVER\", \"BDDI\", \"Unnamed: 0\", \"OVERBDNGRADNT\", \"FRACTUREGRADNT\", \"PWPA\", \"KREV\", \"BDTI\", \"LTHDIGITAL\", \"MTOA\"],\n",
    "    'SPPA': [\"SPPA\", \"MFIA\", \"RPMT\", \"RPMS\", \"ETHA\", \"GWR\", \"U\", \"CALI\", \"K\", \"TQA\", \"TH\", \"MFOA\"],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "TARGET_COL = 'PPE'\n",
    "FEATURE_COLS = target_to_feature[TARGET_COL]\n",
    "lookahead = 50\n",
    "\n",
    "# df = your DataFrame; TARGET = 'KREV'\n",
    "corr_with_target = df.dropna().corr()[TARGET_COL].abs().sort_values(ascending=False)\n",
    "print(\"Top features by |ρ|:\\n\", corr_with_target.head(12))\n",
    "\n",
    "\n",
    "windows = [3,10]\n",
    "depth_col = \"Unnamed: 0\"\n",
    "\n",
    "new_cols = []\n",
    "for w in windows:\n",
    "    roll = (\n",
    "        df[FEATURE_COLS]\n",
    "        .rolling(window=w, min_periods=1)      # past‑only window\n",
    "        .mean()\n",
    "        .add_suffix(f\"_rm{w}\")\n",
    "    )\n",
    "    df = pd.concat([df, roll], axis=1)\n",
    "    new_cols.extend(roll.columns.tolist())\n",
    "\n",
    "FEATURE_COLS = FEATURE_COLS + new_cols\n",
    "\n",
    "FEATURE_COLS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# assume df, FEATURE_COLS and TARGET_COL are already defined\n",
    "depths    = df['Unnamed: 0'].to_numpy()\n",
    "X_mat     = df[FEATURE_COLS].to_numpy()    # shape (n_samples, n_features)\n",
    "y_vec     = df[TARGET_COL].to_numpy()      # shape (n_samples,)\n",
    "lookahead = 50\n",
    "\n",
    "# 1) compute the look‐ahead indices\n",
    "j = np.searchsorted(depths, depths + lookahead)\n",
    "\n",
    "# 2) First mask: keep only those i where j is in‐bounds\n",
    "in_bounds = j < len(depths)\n",
    "i1        = np.nonzero(in_bounds)[0]    # the original row-indices\n",
    "j1        = j[in_bounds]               # the corresponding look-ahead indices\n",
    "\n",
    "# 3) Second mask: drop any whose target is NaN\n",
    "not_nan   = ~np.isnan(y_vec[j1])\n",
    "i2        = i1[not_nan]\n",
    "j2        = j1[not_nan]\n",
    "\n",
    "# 4) slice out your X and y\n",
    "X = X_mat[i2]\n",
    "y = y_vec[j2]\n",
    "\n",
    "print(\"Prepared X.shape =\", X.shape, \"   y.shape =\", y.shape)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "block_size = 2000\n",
    "n = len(X)\n",
    "\n",
    "# assign each row to a “block” and pick evens for train, odds for test\n",
    "blocks = np.arange(n) // block_size\n",
    "train_mask = (blocks % 2) == 0\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_test  = X[~train_mask]\n",
    "y_train = y[train_mask]\n",
    "y_test  = y[~train_mask]\n",
    "\n",
    "\n",
    "from sklearn.pipeline             import Pipeline\n",
    "from sklearn.ensemble            import HistGradientBoostingRegressor\n",
    "from sklearn.metrics             import mean_squared_error\n",
    "\n",
    "# 1) Define your HGBR hyper-parameters\n",
    "hgb_params = {\n",
    "    'max_iter': 500,           # number of trees\n",
    "    'learning_rate': 0.05,      # shrinkage\n",
    "    'max_depth': 5,           # tree depth\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,               # prints progress per iteration\n",
    "    'l2_regularization': 1.0, # helps control overfitting with deeper trees\n",
    "    'n_iter_no_change': 50,\n",
    "}\n",
    "\n",
    "# 2) Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    # note: no scaler needed for tree-based methods\n",
    "    ('hgb', HistGradientBoostingRegressor(**hgb_params))\n",
    "])\n",
    "\n",
    "# 3) Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4) Predict & evaluate\n",
    "y_pred    = pipeline.predict(X_test)\n",
    "test_mse  = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE (HistGradientBoosting):\", test_mse)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 0) Copy so you don’t clobber the original\n",
    "df3 = df.copy()\n",
    "\n",
    "# 1) Pre‑allocate two prediction columns\n",
    "df3['pred_train_50m'] = np.nan\n",
    "df3['pred_test_50m']  = np.nan\n",
    "\n",
    "# 2) Roll your arrays\n",
    "depths   = df3['Unnamed: 0'].to_numpy()\n",
    "X_mat    = df3[FEATURE_COLS].to_numpy()\n",
    "y_vec    = df3[TARGET_COL].to_numpy()\n",
    "lookahead = 50\n",
    "\n",
    "# 3) Look‑ahead index (j[i] ≈ i + lookahead‑metres)\n",
    "j = np.searchsorted(depths, depths + lookahead)\n",
    "\n",
    "# 4) Keep only i,j pairs where j is in bounds\n",
    "in_bounds = j < len(depths)\n",
    "i1 = np.nonzero(in_bounds)[0]\n",
    "j1 = j[in_bounds]\n",
    "\n",
    "# 5) Drop any pair whose target y[j] is NaN\n",
    "not_nan = ~np.isnan(y_vec[j1])\n",
    "i2 = i1[not_nan]\n",
    "j2 = j1[not_nan]\n",
    "\n",
    "# 6) Build your feature/label arrays for prediction\n",
    "X = X_mat[i2]       # features drawn at row i2\n",
    "y =  y_vec[j2]      # “future” targets at row j2\n",
    "\n",
    "# 7) Define alternating‑block train/test mask\n",
    "block_size = 2000\n",
    "blocks     = np.arange(len(X)) // block_size\n",
    "train_mask = (blocks % 2) == 0\n",
    "\n",
    "# 8) Break out the row‑indices for DF write‑back\n",
    "i_train = i2[train_mask]    # feature rows used for training\n",
    "j_train = j2[train_mask]    # rows where pred_train_50m will go\n",
    "i_test  = i2[~train_mask]   # feature rows used for testing\n",
    "j_test  = j2[~train_mask]   # rows where pred_test_50m will go\n",
    "\n",
    "# 9) Run your model on each slice\n",
    "pred_train = pipeline.predict(X[train_mask])\n",
    "pred_test  = pipeline.predict(X[~train_mask])\n",
    "\n",
    "# 10) Write back into the two new columns\n",
    "df3.loc[j_train, 'pred_train_50m'] = pred_train\n",
    "df3.loc[j_test,  'pred_test_50m']  = pred_test\n",
    "\n",
    "\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "fig = px.line(df3, x=\"Unnamed: 0\", y=[TARGET_COL, \"pred_train_50m\", \"pred_test_50m\"],\n",
    "              labels={\"x\": \"Depth\", \"y\": TARGET_COL},\n",
    "              title=f\"{TARGET_COL} vs. Depth\")\n",
    "fig.show()\n",
    "\n",
    "df4 = df3[['Unnamed: 0', 'pred_50m', 'prediction_for_50m']]\n",
    "\n",
    "df4\n",
    "\n",
    "df4.to_csv(f'{TARGET_COL}2.csv', index=False)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(pipeline, f'Models/{TARGET_COL}_50m_pipeline.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ce9ce-5a4f-46fb-bb60-0e08865ee6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
